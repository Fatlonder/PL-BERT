{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1d31f54",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing Wikipedia (English) dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb8ed4",
   "metadata": {},
   "source": [
    "### Initilizing phonemizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c9908",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/fatlonder/sq-wiki/resolve/main/sqwiki-20241001.parquet_clean.parquet ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install espeak-ng -y\n",
    "!python -m pip install pandas pyarrow mwparserfromhell singleton-decorator datasets \"transformers<4.33.3\" accelerate nltk phonemizer sacremoses pebble espeakng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52b79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "from pebble import ProcessPool\n",
    "from concurrent.futures import TimeoutError\n",
    "import yaml\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from phonemize import phonemize\n",
    "import phonemizer\n",
    "from transformers import TransfoXLTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca5ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"Configs/config.yml\" # you can change it to anything else\n",
    "config = yaml.safe_load(open(config_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b363b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_phonemizer = phonemizer.backend.EspeakBackend(language='sq', preserve_punctuation=True,  with_stress=True)\n",
    "tokenizer = TransfoXLTokenizer.from_pretrained(config['dataset_params']['tokenizer']) # you can use any other tokenizers if you want to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb25417",
   "metadata": {},
   "source": [
    "### Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e5ae16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dataset = load_dataset(\"wikipedia\", \"20220301.en\")['train'] # you can use other version of this dataset\n",
    "dataset = load_dataset(\"parquet\", data_files={'train': '/kaggle/working/sqwiki-20241001.parquet_clean.parquet'})\n",
    "\n",
    "!mkdir prcesedsq2\n",
    "\n",
    "root_directory = \"/kaggle/working/prcesedsq2\" # set up root directory for multiprocessor processing\n",
    "input_file = '/kaggle/working/sqwiki-20241001.parquet_clean.parquet'\n",
    "output_file = '/kaggle/working/merged_dataset.parquet'\n",
    "num_shards = 100\n",
    "max_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a578d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shard(i):\n",
    "    try:\n",
    "        df = pd.read_parquet(input_file)\n",
    "        \n",
    "        shard_size = len(df) // num_shards\n",
    "        start_idx = i * shard_size\n",
    "        end_idx = start_idx + shard_size if i != num_shards - 1 else len(df)\n",
    "        \n",
    "        shard = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        shard['processed_text'] = shard['text'].apply(lambda text: phonemize(text, global_phonemizer, tokenizer))\n",
    "        \n",
    "        directory = os.path.join(root_directory, f\"shard_{i}\")\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        shard.to_parquet(os.path.join(directory, 'processed.parquet'))\n",
    "        \n",
    "        print(f\"Shard {i} processed and saved.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to process shard {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f9dcf",
   "metadata": {},
   "source": [
    "#### Note: You will need to run the following cell multiple times to process all shards because some will fail. Depending on how fast you process each shard, you will need to change the timeout to a longer value to make more shards processed before being killed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04261364",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with ProcessPool(max_workers=max_workers) as pool:\n",
    "    future = pool.map(process_shard, range(num_shards), timeout=None)\n",
    "\n",
    "    iterator = future.result()\n",
    "    while True:\n",
    "        try:\n",
    "            result = next(iterator)\n",
    "            print(result)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        except TimeoutError as error:\n",
    "            print(f\"Function took longer than {error.args[1]} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78caee6",
   "metadata": {},
   "source": [
    "### Collect all shards to form the processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0568da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_parquet_files(root_directory, output_file):\n",
    "    shard_dirs = [os.path.join(root_directory, d) for d in os.listdir(root_directory) if os.path.isdir(os.path.join(root_directory, d))]\n",
    "    tables = []\n",
    "    for shard_dir in shard_dirs:\n",
    "        try:\n",
    "            shard_file = os.path.join(shard_dir, 'processed.parquet')\n",
    "            table = pq.read_table(shard_file)\n",
    "            tables.append(table)\n",
    "            print(f\"Loaded {shard_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {shard_file}: {e}\")\n",
    "\n",
    "    if tables:\n",
    "        combined_table = pa.concat_tables(tables)\n",
    "        pq.write_table(combined_table, output_file)\n",
    "        print(f\"Merged dataset saved to {output_file}\")\n",
    "    else:\n",
    "        print(\"No tables were loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1547f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_parquet_files(root_directory, output_file)\n",
    "df[['input_ids', 'phonemes']] = df['processed_text'].apply(pd.Series)\n",
    "new_df = df.apply(pd.Series)[['id', 'url', 'title', 'input_ids', 'phonemes']]\n",
    "new_df.to_parquet('sq-wiki-text-phonem-training-20241001.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce886d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from simple_loader import FilePathDataset, build_dataloader\n",
    "\n",
    "dataset = load_dataset(\"parquet\", data_files={'train': '/content/sq-wiki-text-phonem-training-20241001.parquet'})['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf6f6f6",
   "metadata": {},
   "source": [
    "### Remove unneccessary tokens from the pre-trained tokenizer\n",
    "The pre-trained tokenizer contains a lot of tokens that are not used in our dataset, so we need to remove these tokens. We also want to predict the word in lower cases because cases do not matter that much for TTS. Pruning the tokenizer is much faster than training a new tokenizer from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cec407",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = FilePathDataset(dataset)\n",
    "loader = build_dataloader(file_data, num_workers=2, batch_size=8)\n",
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7504eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token = config['dataset_params']['word_separator']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb44a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all unique tokens in the entire dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "unique_index = [special_token]\n",
    "for _, batch in enumerate(tqdm(loader)):\n",
    "    unique_index.extend(batch)\n",
    "    unique_index = list(set(unique_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1445662d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get each token's lower case\n",
    "\n",
    "lower_tokens = []\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    if word.lower() != word:\n",
    "        t = tokenizer.encode([word.lower()])[0]\n",
    "        lower_tokens.append(t)\n",
    "    else:\n",
    "        lower_tokens.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_tokens = (list(set(lower_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a76cda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redo the mapping for lower number of tokens\n",
    "\n",
    "token_maps = {}\n",
    "for t in tqdm(unique_index):\n",
    "    word = tokenizer.decode([t])\n",
    "    word = word.lower()\n",
    "    new_t = tokenizer.encode([word.lower()])[0]\n",
    "    token_maps[t] = {'word': word, 'token': new_t}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c94be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(config['dataset_params']['token_maps'], 'wb') as handle:\n",
    "    pickle.dump(token_maps, handle)\n",
    "print('Token mapper saved to %s' % config['dataset_params']['token_maps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e968e",
   "metadata": {},
   "source": [
    "### Test the dataset with dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9025e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import build_dataloader\n",
    "\n",
    "train_loader = build_dataloader(dataset, batch_size=32, num_workers=0, dataset_config=config['dataset_params'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70874215",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (words, labels, phonemes, input_lengths, masked_indices) = next(enumerate(train_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BERT",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
